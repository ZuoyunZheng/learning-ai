## CUDA (Compute Unified Device Architecture)
The motivation for this page is that I want to get a basic understanding of
how GPUs work from a programming perspective. I can understand that there are
libraries like Torch and TensorFlow that can do this for us but is just seems
like magic to me at the moment.

CUDA is a parallel computing platform and programming model developed by Nvidia
for general computing on its own GPUs (graphics processing units).

### CUDA Ecosystem
```
Dev tools:     NVIDIA SMI  Data Center GPU Mgr  GPU REST Engine

Libraries:     cuBlas   cuFFT  cuSPARSE  cuSOLVER  AGM-X
               Thrust   CUB    cuDNN     cuRand    NCCL

Compilers:     nvcc,nvc  CUDA-GDB  NVIDIA Nsight NVIDIA Visual Profiler PAPI CUDA
               nvc++
               nvfortran

Programming:   CUDA       OpenMP API   OpenACC   OpenCL   PyCUDA
models

Drivers:       Linux and Windows device drivers and runtime (no mac?)
```

`CUB` (CUDA UnBound) is a library of high-performance primitives for CUDA.
`AMG-X` (Adaptive General Matrix eXponentiation)
`NCCL` (NVIDIA Collective Communications Library) is a library that provides
`multi-GPU` and `multi-node` collective communication primitives.

### Compilation units
A CUDA program would have a `.cu` suffix. The nvcc compiler will separate out
the host (CPU) code from the device (GPU) code. The host code is compiled by
the host compiler (gcc, clang, etc.) and the device code is compiled by the
NVIDIAS proprietary backend compiler named PTXAS into an intermediate
representation (PTX) which is later converted into binary code for the GPU.

The host will call code on the GPU using something that is called kernel calls.
A kernel is a function that is executed on the GPU. Each kernel functions runs
the same code but on different data.

```c++
__global__ void add_arrays(int *a, int *b, int *c, int size) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < size) {
        c[tid] = a[tid] + b[tid];
    }
}
```
The `__global__` keyword is used to indicate that this function is a kernel
function. The `add_arrays` function will be executed on the GPU. The `tid`
variable is the thread id. The `blockIdx.x` is the block id and the
`threadIdx.x` is the thread id within the block. The `blockDim.x` is the
number of threads in a block. The `blockIdx.x * blockDim.x + threadIdx.x`
is the global thread id.

There is an example in [array_add](../gpu/cuda/src/array_add.cu) that shows the
above example.

### Parallel Thread Execution (PTX)
When you compile CUDA code with nvcc, the device code doesn't immediately get
translated to machine code. Instead, it first gets compiled to this intermediate
PTX format. The abstraction allows CUDA code to be compiled and then later
be translated into the binary code of a specific GPU.

```console
$ cd gpu/cuda
$ make array-add-ptx
nvcc -ptx src/array-add.cu
//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35059454
// Cuda compilation tools, release 12.6, V12.6.85
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_52
.address_size 64

	// .globl	_Z10add_arraysPiS_S_i
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.global .align 1 .b8 $str[52] = {98, 108, 111, 99, 107, 73, 100, 120, 46, 120, 32, 61, 32, 37, 100, 44, 32, 98, 108, 111, 99, 107, 68, 105, 109, 46, 120, 32, 61, 32, 37, 100, 44, 32, 116, 104, 114, 101, 97, 100, 73, 100, 120, 46, 120, 32, 61, 32, 37, 100, 10};
.global .align 1 .b8 $str$1[45] = {91, 71, 80, 85, 93, 32, 97, 114, 114, 97, 121, 32, 105, 110, 100, 101, 120, 32, 91, 37, 100, 93, 58, 32, 97, 100, 100, 105, 110, 103, 32, 37, 100, 32, 43, 32, 37, 100, 32, 61, 32, 37, 100, 10};

.visible .entry _Z10add_arraysPiS_S_i(
	.param .u64 _Z10add_arraysPiS_S_i_param_0,
	.param .u64 _Z10add_arraysPiS_S_i_param_1,
	.param .u64 _Z10add_arraysPiS_S_i_param_2,
	.param .u32 _Z10add_arraysPiS_S_i_param_3
)
{
	.local .align 16 .b8 	__local_depot0[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<18>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [_Z10add_arraysPiS_S_i_param_0];
	ld.param.u64 	%rd3, [_Z10add_arraysPiS_S_i_param_1];
	ld.param.u64 	%rd4, [_Z10add_arraysPiS_S_i_param_2];
	ld.param.u32 	%r2, [_Z10add_arraysPiS_S_i_param_3];
	add.u64 	%rd5, %SP, 0;
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	st.local.v2.u32 	[%rd1], {%r4, %r3};
	mov.u32 	%r5, %tid.x;
	st.local.u32 	[%rd1+8], %r5;
	mov.u64 	%rd6, $str;
	cvta.global.u64 	%rd7, %rd6;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd7;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd5;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r6, [retval0+0];
	} // callseq 0
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	cvta.to.global.u64 	%rd11, %rd3;
	add.s64 	%rd12, %rd11, %rd9;
	ld.global.u32 	%r7, [%rd12];
	ld.global.u32 	%r8, [%rd10];
	cvta.to.global.u64 	%rd13, %rd4;
	add.s64 	%rd14, %rd13, %rd9;
	add.s32 	%r9, %r7, %r8;
	st.global.u32 	[%rd14], %r9;
	ld.global.u32 	%r10, [%rd12];
	ld.global.u32 	%r11, [%rd10];
	st.local.v4.u32 	[%rd1], {%r1, %r11, %r10, %r9};
	mov.u64 	%rd15, $str$1;
	cvta.global.u64 	%rd16, %rd15;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd16;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd5;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r12, [retval0+0];
	} // callseq 1

$L__BB0_2:
	ret;

}
```

### CUBIN (CUDA Binary)
Is an ELF-formatted file. This contains CUDA executable code sections and
sections containing symbols, relocators, debug info. nvcc embeds cubin files
into the host executable file.

### nvcc
nvcc is the CUDA compiler driver (think gcc or clang). It is used to compile
CUDA programs. nvcc accepts a range of conventional compiler options, such as
for defining macros and include/library paths, and for steering the compilation
process. nvcc also accepts a range of CUDA-specific options for defining the
virtual architecture targeting which the CUDA program is compiled, and for
defining the memory model used, and for steering the compilation process.

### Questions
So when we are going to train a large language model on a GPU we first need to
load the model weights into the host's memory, then copy them over to the GPU's
memory, and then call a kernel function to execute. Is this how it is done in
practice or are there ways to avoid the memory copying?

When training large language models (or any deep learning models) on GPUs, the
model's weights and the data do need to reside in the GPU's memory. However, the
process is a bit more nuanced than just loading everything into the host's
memory and then copying it to the GPU's memory. Here's a breakdown of how it
typically works in practice:

Model Initialization:

When you initialize a model using deep learning frameworks like TensorFlow or
PyTorch, and you've set the device to a GPU, the model's weights are often
directly initialized in the GPU's memory. There's no need to first initialize
them on the CPU and then transfer them.
When you instruct these frameworks to initialize tensors (or model parameters)
on the GPU, a series of steps occur:

* The framework communicates with the GPU through a driver API (e.g., CUDA for
NVIDIA GPUs).
* Memory on the GPU is allocated to store the tensor.
* Initialization operations (like random number generation for weight
initialization) are executed as GPU kernels. These operations fill the allocated
memory with the initial values.

The key takeaway is that these operations occur directly on the GPU without the
need for an intermediary step on the CPU.


Data Loading and Batching:


Training data is usually read in batches. Instead of loading the entire dataset
into the host's memory and then transferring it to the GPU, data is typically
loaded batch-by-batch. Each batch is transferred to the GPU just before it's
needed for training.
Modern deep learning frameworks and data loaders handle this process
efficiently, often using asynchronous operations to overlap data loading on the
CPU with computation on the GPU.

Once the model's weights are on the GPU, they typically stay there throughout
the training process. Forward passes, backward passes, and weight updates all
happen on the GPU. The weights aren't constantly moved back and forth between
the host and the GPU.
It's only if you need to save the model's weights or inspect them on the CPU
that you'd transfer them back to the host's memory.

### Streams
A stream in CUDA is a sequence of commands that execute in order. It is like a
queue of commands that are executed one after the other. Now, what I've been 
doing in my examples is just using synchronous commands, like `cudaMemcpy` and
and not specifying a stream when calling a kernel. When we call a function
like `cudaMemcpy` this is a synchronous operation, the host program will not
progress (it will block) until the entire memory transfer is complete.
When we call `cudaMemcpy` this function is added to the default CUDA stream. The
default stream operations are all executed in the order they are called/added.
In this way `cudaMemcpy` acts as a synchronization point between the host and
the device.

When using streams we use the async version of memcpy, like `cudaMemcpyAsync`,
and for launching a kernel we specify a stream as an argument.

An example can be found in [streams.cu](../gpu/cuda/src/streams.cu).

###  CMAKE_CUDA_FLAGS
This is a cmake variable that is passed to `nvcc` when compiling CUDA code.

#### FASTFP16_AVAILABLE
This flag tells nvcc to enable the FP16 code paths in the CUDA code.
```console
$ cmake -S . -B build -DGGML_CUDA=ON -DCMAKE_CUDA_FLAGS="-DFASTFP16_AVAILABLE"
```

### Compute Compability (CC)
https://developer.nvidia.com/cuda-gpus

I have have a `GeForce RTX 4070` which has a compute capability of `8.9`.
```console
$ ./minimal
CUDA Runtime version: 12.6
CUDA Driver version: 12.6
CUDA device count: 1
Device 0 - Total VRAM: 11.62 GB

Device 0:
  Name: NVIDIA GeForce RTX 4070
  Compute Capability: 8.9
  Multiprocessors: 46
  Clock Rate: 2505 MHz
  Total Global Memory: 11.62 GB
  L2 Cache Size: 36.00 MB
CUDA program ran successfully
```

CC 8.0, 8.6: Ampere (RTX 30 series, A100)
CC 8.9:      Ada Lovelace (RTX 40 series)
CC 9.0:      Hopper (H100)

The compute capability is something that can be set on the command line as the
`-arch` flag. For example:
```console
$ nvcc -arch=sm_89 -o $@ $<
```
Where `sm` stands for streaming multiprocessor and `89` is the compute
capability. Using this tells the compiler to generate code that is optimized
these specific GPUs.

### NPPC (NVIDIA Performance Primitives Core)
This is a collection of GPU-accelerated image, video, and signal processing
functions.
