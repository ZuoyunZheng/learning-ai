## Language Model Security

There are various security concerns when using language models and how they are
used today. Things like prompt injection, poisoning of training data, DoS
attacks, supply chain attacks (like using thirdparty datasets or pre-trained
models, and more.

### Model Collapse
This is something that occurs when more and more content on the internet is
generated by AI, and then new models are trained this new content. The new
models become too dependent on patterns that are present in the generated data,
and they are replicating patterns they have already seen.
So probable events are overestimated and improbable events are underestimated
and as this happens over generations of models this problem amplifies.

### Resources
* https://github.com/chawins/llm-sp
* https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf


### Prompt Injection
This is when an attacker injects a prompt into the model that will cause the
model to by passing the models system prompt, like getting it to respond to
queries that it should not be able to respond to. This is called "jailbreaking".

The actual prompt might not come from an attacker directly but could be on a
web site controlled by the attacker. For example, a user might ask the LLM to
summarize a web page which contains the prompt.

Another example that I had not thought of a system prompt could be inserted into
a resume and when that is used in an LLM it will always respond that the
candidate is a great match for the position being applied for. Detail of such
and attach can be found here: https://kai-greshake.de/posts/inject-my-pdf/


### Insecure output handling
This is about handling the output from the LLM in a secure way. For example, one
should not use the output directly to execute something on the command line org
evalutate it as code without some sort of validation.

### Training data poisoning
So this is pretty interesting as this could be as simple as having a webpage
with incorrect information, or perhaps multiple with slightly different content.
Without knowing what data the base model was trained on, it is hard to know if
the data is correct or not. This is a problem with the current state of LLMs
as very few model builders/trainers publish this information. For example,
ChatGPT-4, Claude, Llama do not publish this information. Falcon actually do
and I should find which others might do this. Also there are models that are
fine-tuned using those base models and this applies to them as well. So it may
be important to consider the base model used. But even if one knows, say we use
Falcon as the base model, is it feasible to verify the training data considering
that it much be a huge amount of data?  
So for Falcon it is trained on a dataset named [RefinedWeb] which is a refined
version of CommonCrawl. 
There are other sources like the [Pile]

So the training data used should be known and it part of the ml-bom.

Large-scale Artificial Intelligence Open Network (LAION) is a non-profit org
which provides datasets.


[rerfinedweb]: https://huggingface.co/datasets/tiiuae/falcon-refinedweb
[pile]: https://pile.eleuther.ai/

### Verifing correctness of the model
How will it be possible to verify the correctness of a model once we have AGI?  
Only a few humans might have detailed knowledge of a subject but not the
majority. And if we get super human AGI we will have no way at all to verify
if the output is correct.

### LLM Attacks
https://github.com/llm-attacks/llm-attacks
