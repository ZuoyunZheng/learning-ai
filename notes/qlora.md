## Quantized Low-Rank Adaptation
This is an extension of [lora](lora.md) that uses quantization to reduce the
memory footprint of the model. This is done by quantizing the weights of the
model to 8-bit integers. 
