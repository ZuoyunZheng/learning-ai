## Large Language and Vision Assistent
LLaVA is an open-source chatbot/assistent trained by fine-tuning LLaMA/Vicuna on
GPT-generated multimodal instruction-following data. So it is able to take in
an image and a prompt and it will answer them (this is the instruction part).

An interesting part of the learning process is that each image needs some text
to go with it, but how do we get that text?  
What was done in the training of LLaVA was that the text was generated by
a GPT (ChatGPT?). So this is a text only GPT remember so that it cannot handle
the image itself. 

There are annotated data sets like [Coco](https://cocodataset.org/) where humans
have provided the annotations for the image. So the image is described in text
format, which consists of bounding boxes (like where things are in the image
with coordinates) and such to describe what is in the image. So we have an
image, and the caption, and the description of the image with the bounding boxes
and with this the LLM can understand the image.

The most recent version of LLaVA is
[LLaVA v1.6/LLaVA-Next](https://llava-vl.github.io/blog/2024-01-30-llava-next/).
And the paper for 1.5 can be found [here](https://arxiv.org/abs/2310.03744).

Llava has a LLM, a Visual Transformer (CLIP with ViT-L/14 (ViT Large and using
14x14 patches), and adds a trainable projection layer (Multi-layer projector in
LLaVA-1.5). The projection layer is used to project/transform the patch
embeddings into the token embedding space.

So a [ViT](vit.md) will produce a patch embedding for the image, and the LLM
will produce token embeddings:
```
   +-------------+      +----------------+
   | Text input  |----->| Vicuna         |---> token embeddings
   +-------------+      +----------------+

   +-------------+      +----------------+
   | Image input |----->| ViT-L/14       |--->  patch embeddings
   +-------------+      +----------------+
```
But these are embeddings from different embedding spaces, so we need to project
them into the same space. This is done by a trainable projection layer (W):
```
   patch embeddings [0                 511]
       (Z)

Zᵥ = the patch embeddings

    Hᵥ = W * Zᵥ

Hᵥ = patch embeddings but now in the same space as the token embeddings
W  = trainable projection layer
```

Now, during training the input data set consists of pairs of images and text
inputs. At inference time there might not be any text, we could just pass an
image and have the model generate a response for it, perhaps describing what
it sees in the image. But we could also pass a prompt along with the image and
ask a specific question about the image.

For example, we can pass the following image to `llava-cli` which is part of
llama.cpp and it will describe it:

![image](apollo11.jpg)

```console
$ ~/work/ai/llama.cpp/llava-cli --no-display-prompt --log-disable --n-gpu-layers 25 -m ~/work/ai/llava-v1.5-7b/ggml-model-f16.gguf --mmproj ~/work/ai/llava-v1.5-7b/mmproj-model-f16.gguf --image ~/work/ai/learning-ai/notes/apollo11.jpg 

The image features a man wearing a white space suit, standing on the moon and posing in front of an American flag. He is carrying several items in his suit, including a backpack and two handheld objects. There are also a couple of additional smaller figures visible in the distance, likely also part of the same scene. The overall atmosphere suggests that this picture was taken during a historical moon landing event or space exploration mission, capturing the moment as mankind achieved another milestone in its cosmic journey.
```

And we can also pass in a prompt along with the image and ask a specific
question:
```console
$ ~/work/ai/llama.cpp/llava-cli --no-display-prompt --log-disable --n-gpu-layers 25 -m ~/work/ai/llava-v1.5-7b/ggml-model-f16.gguf --mmproj ~/work/ai/llava-v1.5-7b/mmproj-model-f16.gguf --image ~/work/ai/learning-ai/notes/apollo11.jpg -p "Is there a banan in the image?"

No, there is no banana in the image.
```

During training the first stage trains the projection layer W and the ViT and
LLM are frozen (not updated). So this is mostly about enabling the patch
embeddings to be projected into the same space as the token embeddings.
In the second stage the projector is trained as well as the language model which
is about the instruction tuning learning.



### llama.cpp example
First we clone https://huggingface.co/liuhaotian/llava-v1.5-7b which is the
instruction tuned model, which recall was trained on prompts which contained
a textual description of an image and then user/assistent interactions, and also
with the images. This is what produces the projector.

I did the following in the directory above my checked out llama.cpp directory.
We need to checkout the LLaVA model:
```console
$ git clone https://huggingface.co/liuhaotian/llava-v1.5-7b
```

And we need the Vision Transformer (ViT) model:
```console
$ git clone https://huggingface.co/openai/clip-vit-large-patch14-336
```

Create a Python virtual environment and install the required packages:
```console
$ python3.11 -m venv llava-venv
$ source llava-venv/bin/activate
(llava-venv) $ pip install torch numpy gguf transformers pillow sentencepiece
```
Then we can run the script llava-surgery.py script:
```console
(llava-venv) $ python examples/llava/llava-surgery.py -m ../llava-v1.5-7b/
Done!
Now you can convert ../llava-v1.5-7b/ to a a regular LLaMA GGUF file.
Also, use ../llava-v1.5-7b//llava.projector to prepare a llava-encoder.gguf file.
```
What this script does is that it looks up the pretrained PyTorch weight files
(the last one) which in my case is pytorch_model-00002-of-00002.bin. Hmm, could
this not be looked up instead using the pytorch_model.bin.index.json?  
The following tensors are retrieved and stored:
```console
"model.mm_projector.0.bias": "pytorch_model-00002-of-00002.bin",               
"model.mm_projector.0.weight": "pytorch_model-00002-of-00002.bin",             
"model.mm_projector.2.bias": "pytorch_model-00002-of-00002.bin",               
"model.mm_projector.2.weight": "pytorch_model-00002-of-00002.bin", 
```
Then `torch.save` (which would be in pickle format) is used which will save the
projector weights in a files called llava.projector:
```python
torch.save(projector, f"{args.model}/llava.projector")
```
```console
$ cd fundamentals/python && source fund/bin/activate
(fund) $ python src/list-pytorch-model.py 
model.mm_projector.0.weight: torch.Size([4096, 1024])
model.mm_projector.0.bias: torch.Size([4096])
model.mm_projector.2.weight: torch.Size([4096, 4096])
model.mm_projector.2.bias: torch.Size([4096])
```
The surgery script also removes these weights from the model file and saves it,
so we need to revert the changes in ../llava-v1.5-7b/ before we can run the
script again.
So at this point pytorch_model-00002-of-00002.bin does not contain the tensor
weights related to the projector.

The `llava.projector` are the tensors that projector which we will use with the
image encoder so that it can convert/transform the image embeddings into the
same space as the text embeddings.

And the type of model has to be GGUF so this will also convert it to that format:
```console
(llava-venv) $ python ./examples/llava/convert-image-encoder-to-gguf.py -m ../clip-vit-large-patch14-336 --llava-projector ../llava-v1.5-7b/llava.projector --output-dir ../llava-v1.5-7b
gguf: This GGUF file is for Little Endian only
Projector tensors added
...
Done. Output file: ../llava-v1.5-7b/mmproj-model-f16.gguf
```
So that is the ViT with the addition of the projector tensors, converted to
GGUF format which is now in `mmproj-model-f16.gguf`

Then we need to convert the llava part of llava to GGUF format, which we removed
the projector tensors from:
```console
(llava-venv) $ python ./convert.py ../llava-v1.5-7b
Loading model file ../llava-v1.5-7b/pytorch_model-00001-of-00002.bin
Loading model file ../llava-v1.5-7b/pytorch_model-00001-of-00002.bin
Loading model file ../llava-v1.5-7b/pytorch_model-00002-of-00002.bin
params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=None, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../llava-v1.5-7b'))
Found vocab files: {'tokenizer.model': PosixPath('../llava-v1.5-7b/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': None}
Loading vocab file '../llava-v1.5-7b/tokenizer.model', type 'spm'
Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>
...
Writing ../llava-v1.5-7b/ggml-model-f16.gguf, format 1
Ignoring added_tokens.json since model matches vocab size without it.
gguf: This GGUF file is for Little Endian only
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
gguf: Setting special token type pad to 0
gguf: Setting add_bos_token to True
gguf: Setting add_eos_token to False
...
Wrote ../llava-v1.5-7b/ggml-model-f16.gguf
```

So the removal of the projector tensors from the model confused me somewhat, 
I understand that they need to be added to the ViT model, but I don't understand
why they need to be removed from the LLaMA model. Without removing them we would
run into and error when converting the model:
```console
(llava-venv) $ python ./convert.py ../llava-v1.5-7b
...
model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]
model.norm.weight                                -> output_norm.weight                       | F16    | [4096]
Traceback (most recent call last):
  File "/home/danielbevenius/work/ai/llama.cpp/./convert.py", line 1483, in <module>
    main()
  File "/home/danielbevenius/work/ai/llama.cpp/./convert.py", line 1469, in main
    model   = convert_model_names(model, params, args.skip_unknown)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/danielbevenius/work/ai/llama.cpp/./convert.py", line 1206, in convert_model_names
    raise Exception(f"Unexpected tensor name: {name}. Use --skip-unknown to ignore it (e.g. LLaVA)")
Exception: Unexpected tensor name: model.mm_projector.0.weight. Use --skip-unknown to ignore it (e.g. LLaVA)
```
This is because these tensors are not defined in gguf-py/gguf/tensor_mapping.py
so they will not be recognized, hence the error. I think it would be alright to
remove the removal of the projector tensors from the LLaMA model and then added
the `--skip-unknown` flag to the convert.py script.

Using `--skip-unknown` as suggested seems to work:
```console
$ python ./convert.py ../llava-v1.5-7b --skip-unknown
```
Perhaps this could be changed as it would be nice to be able to not have to
update the original model file.

Now, we can pass the following image to `llava-cli` and it will describe it:

![image](apollo11.jpg)

```console
(llava-venv) $ ./llava-cli --no-display-prompt --log-disable --n-gpu-layers 25 -m ../llava-v1.5-7b/ggml-model-f16.gguf --mmproj ../llava-v1.5-7b/mmproj-model-f16.gguf --image apollo11.jpg
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes
clip_model_load: model name:   openai/clip-vit-large-patch14-336
clip_model_load: description:  image encoder for LLaVA
clip_model_load: GGUF version: 3
clip_model_load: alignment:    32
clip_model_load: n_tensors:    377
clip_model_load: n_kv:         19
clip_model_load: ftype:        f16

clip_model_load: loaded meta data with 19 key-value pairs and 377 tensors from ../llava-v1.5-7b/mmproj-model-f16.gguf
clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
clip_model_load: - kv   0:                       general.architecture str              = clip
clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
clip_model_load: - kv   4:                          general.file_type u32              = 1
clip_model_load: - kv   5:                               general.name str              = openai/clip-vit-large-patch14-336
clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA
clip_model_load: - kv   7:                        clip.projector_type str              = mlp
clip_model_load: - kv   8:                     clip.vision.image_size u32              = 336
clip_model_load: - kv   9:                     clip.vision.patch_size u32              = 14
clip_model_load: - kv  10:               clip.vision.embedding_length u32              = 1024
clip_model_load: - kv  11:            clip.vision.feed_forward_length u32              = 4096
clip_model_load: - kv  12:                 clip.vision.projection_dim u32              = 768
clip_model_load: - kv  13:           clip.vision.attention.head_count u32              = 16
clip_model_load: - kv  14:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010
clip_model_load: - kv  15:                    clip.vision.block_count u32              = 23
clip_model_load: - kv  16:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]
clip_model_load: - kv  17:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]
clip_model_load: - kv  18:                              clip.use_gelu bool             = false
clip_model_load: - type  f32:  235 tensors
clip_model_load: - type  f16:  142 tensors
clip_model_load: CLIP using CUDA backend
clip_model_load: text_encoder:   0
clip_model_load: vision_encoder: 1
clip_model_load: llava_projector:  1
clip_model_load: model size:     595.53 MB
clip_model_load: metadata size:  0.14 MB
clip_model_load: params backend buffer size =  595.53 MB (377 tensors)
clip_model_load: compute allocated memory: 36.18 MB
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../llava-v1.5-7b/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.22 MiB
llm_load_tensors: offloading 25 repeating layers to GPU
llm_load_tensors: offloaded 25/33 layers to GPU
llm_load_tensors:        CPU buffer size = 12853.02 MiB
llm_load_tensors:      CUDA0 buffer size =  9650.78 MiB
...................................................................................................
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =   224.00 MiB
llama_kv_cache_init:      CUDA0 KV buffer size =   800.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:  CUDA_Host input buffer size   =    12.01 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   171.60 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =   167.20 MiB
llama_new_context_with_model: graph splits (measure): 5

encode_image_with_clip: image encoded in    84.79 ms by CLIP (    0.15 ms per image patch)

 In the image, a person wearing a white and red space suit stands on top of a moon surface. They have their back turned to the American flag that is waving nearby. The person appears to be an astronaut or possibly an alien character in an iconic scene from science fiction movies. They are holding a suitcase, which could potentially be carrying supplies for their journey or a prop from the movie set.

llama_print_timings:        load time =    6637.59 ms
llama_print_timings:      sample time =      56.21 ms /    87 runs   (    0.65 ms per token,  1547.82 tokens per second)
llama_print_timings: prompt eval time =    5576.01 ms /   616 tokens (    9.05 ms per token,   110.47 tokens per second)
llama_print_timings:        eval time =   16764.83 ms /    87 runs   (  192.70 ms per token,     5.19 tokens per second)
llama_print_timings:       total time =   27646.54 ms /   703 tokens
```

### Using LLaVA 1.6
First clone the LLaVA 1.6 model:
```console
$ git clone -v --progress --depth 1 --single-branch --branch main https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b
```
One thing to note about this model is that it includes the vision part in the    
model files, in contrast to llava-1.5 where the vision part is in a separate     
model that we checked out. So the surgery script for this version will extract
them.

Then we run the llava-surgery-v2.py script:
```console
$ python examples/llava/llava-surgery-v2.py -C -m ../llava-v1.6-vicuna-7b/
```
Then copy the following files to a new directory:
```console
$ mkdir vit
$ cp ../llava-v1.6-vicuna-7b/llava.clip vit/pytorch_model.bin
$ cp ../llava-v1.6-vicuna-7b/llava.projector vit/
$ curl -s -q https://huggingface.co/cmp-nct/llava-1.6-gguf/raw/main/config_vit.json -o vit/config.json

$ python ./examples/llava/convert-image-encoder-to-gguf.py -m vit --llava-projector vit/llava.projector --output-dir vit --clip-model-is-vision
```
Then we can convert the model to gguf format:
```console
$ python ./convert.py ../llava-v1.6-vicuna-7b/
```
And finally we can run the llava-cli using the 1.6 model version:
```console
~/work/ai/llama.cpp/llava-cli --no-display-prompt --log-disable --n-gpu-layers 25 -m ~/work/ai/llava-v1.6-vicuna-7b/ggml-model-f16.gguf --mmproj ~/work/ai/llama.cpp/vit/mmproj-model-f16.gguf --image ~/work/ai/learning-ai/notes/apollo11.jpg
...

The image shows an astronaut standing on the surface of the moon, looking towards the camera. He is wearing a white space suit with the American flag patch visible on his chest, and he has a backpack strapped to his shoulders. In front of him stands a small wooden pole with an American flag attached to it. This scene depicts a historical moment from the Apollo missions when astronauts planted flags on the moon as part of their mission objectives. The environment around them is barren and rocky, characteristic of the moon's surface.
```

### BakLLaVA-1
This is a Mistral 7B base model agumented with LLaVA-1.5 architecture compared
to the version above which was based on Vicuna (which recall is a fine-tuned
LLaMA base model for chat (conversations were collected from ShareGPT)).

First clone BakLLaVA-1:

```console
$ git clone https://huggingface.co/SkunkworksAI/BakLLaVA-1
```

Then we run the llava-surgery.py script:
```console
(llava-venv) $ python examples/llava/llava-surgery.py -m ../BakLLaVA-1/
Done!
Now you can convert ../BakLLaVA-1/ to a regular LLaMA GGUF file.
Also, use ../BakLLaVA-1//llava.projector to prepare a llava-encoder.gguf file.
```

Then we convert the vision model to gguf format:
```console
(llava-venv) $ python ./examples/llava/convert-image-encoder-to-gguf.py -m ../clip-vit-large-patch14-336 --llava-projector ../BakLLaVA-1/llava.projector --output-dir ../BakLLaVA-1
...
Done. Output file: ../BakLLaVA-1/mmproj-model-f16.gguf
```

Then we convert the BakLLaVA-1 to gguf format:
```console
$ python ./convert.py ../BakLLaVA-1
...
Wrote ../BakLLaVA-1/ggml-model-f16.gguf
```

And then we can run the llava-cli using the BakLLaVA-1 model:
```console
(llava-venv) $ ~/work/ai/llama.cpp/llava-cli --no-display-prompt --log-disable --n-gpu-layers 25 -m ~/work/ai/BakLLaVA-1/ggml-model-f16.gguf --mmproj ~/work/ai/BakLLaVA-1/mmproj-model-f16.gguf --image ~/work/ai/learning-ai/notes/apollo11.jpg
...

The image is a photograph of an astronaut standing on the surface of the Moon during the Apollo 11 mission.
```

### Lightweight Downsampling Projector (LDP)
This is an option that is available in the script.
```console
(llava-venv) $ python examples/llava/convert-image-encoder-to-gguf.py --help
usage: convert-image-encoder-to-gguf.py [-h] -m MODEL_DIR [--use-f32] [--text-only] [--vision-only]
                                        [--clip_model_is_vision] [--llava-projector LLAVA_PROJECTOR]
                                        [--projector-type {mlp,ldp}]
                                        [--image-mean IMAGE_MEAN IMAGE_MEAN IMAGE_MEAN]
                                        [--image-std IMAGE_STD IMAGE_STD IMAGE_STD] [-o OUTPUT_DIR]
                                        [--image_mean IMAGE_MEAN [IMAGE_MEAN ...]]
                                        [--image_std IMAGE_STD [IMAGE_STD ...]]

options:
  -h, --help            show this help message and exit
  -m MODEL_DIR, --model-dir MODEL_DIR
                        Path to model directory cloned from HF Hub
  --llava-projector LLAVA_PROJECTOR
                        Path to llava.projector file. If specified, save an image encoder for LLaVA models.
  --projector-type {mlp,ldp}
                        Type of projector. Possible values: mlp, ldp
  ...
```

https://arxiv.org/pdf/2402.03766.pdf


### llama.cpp llava notes
This section will step through the llava-cli example to understand how it work.

First we will build the llama.cpp project with debugging symbols enabled and
with CUDA support:
```console
$ cmake -S . -B build -DGGML_CUDA=On -DCMAKE_BUILD_TYPE=Debug
$ cmake --build build
```
Lets try running the example first to see that it works as expected:
```console
$ ./build/bin/llama-llava-cli -m models/vicuna-7b-q5_k.gguf --mmproj models/mmproj-vicuna7b-f16.gguf --image ~/work/ai/learning-ai/notes/apollo11.jpg -c 4096 -ngl 15
```
This will output the following (cleaned up from debugging output):
```console
The image you've shared depicts an astronaut on the surface of the moon,
standing next to a flag of the United States. This photograph captures a
significant moment in history, as the Apollo missions were the first time that
humans walked on the moon and planted the flag of the United States. The image
is often remembered as a symbol of national pride and the triumph of human
ingenuity. It was taken on July 20, 1969, during the Apollo 11 mission.
```
The actual output was this which I'm saving here so that I can take a look at
where these are coming from:
```console
 The image you've shared depicts an astronaut on the surface of the moon, standing next to a flagggml_gallocr_needs_realloc: src 1 (KQ_mask) of node kq_soft_max_ext-0 is not valid
ggml_gallocr_alloc_graph: cannot reallocate multi buffer graph automatically, call reserve
ggml_backend_sched_alloc_splits: failed to allocate graph, reserving (backend_ids_changed = 0)
 of the United States. This photograph captures a significant moment in history, as the Apollo missions were the first time that humans walked on the moon andggml_gallocr_needs_realloc: node kq-0 is not valid
ggml_gallocr_alloc_graph: cannot reallocate multi buffer graph automatically, call reserve
ggml_backend_sched_alloc_splits: failed to allocate graph, reserving (backend_ids_changed = 0)
 planted the flag of the United States. The image is often remembered as a symbol of national pride and the triumph of human ingenuity. It was takenggml_gallocr_needs_realloc: node kq-0 is not valid
ggml_gallocr_alloc_graph: cannot reallocate multi buffer graph automatically, call reserve
ggml_backend_sched_alloc_splits: failed to allocate graph, reserving (backend_ids_changed = 0)
 on July 20, 1969, during the Apollo 11 mission.
llama_perf_context_print:        load time =   36031.05 ms
llama_perf_context_print: prompt eval time =   30555.63 ms /  2920 tokens (   10.46 ms per token,    95.56 tokens per second)
llama_perf_context_print:        eval time =   54394.93 ms /   109 runs   (  499.04 ms per token,     2.00 tokens per second)
llama_perf_context_print:       total time =   90945.80 ms /  3029 tokens
```
Great, so lets start stepping through the code:
```console
$ gdb --args ./build/bin/llama-llava-cli -m models/vicuna-7b-q5_k.gguf --mmproj models/mmproj-vicuna7b-f16.gguf --image ~/work/ai/learning-ai/notes/apollo11.jpg -c 4096 -ngl 15
(gdb) br llava-cli.cpp:273
Breakpoint 1 at 0x5e566: file /home/danbev/work/ai/llama.cpp/examples/llava/llava-cli.cpp, line 273.
```
Skipping the parsing of command line arguments and some checks we get to:
```c++
    auto * model = llava_init(&params);
```
This will load the language model which in our case is the Vicuna 7B model. This
uses common.cpp to load and is something we have gone through before.
```c
static struct llama_model * llava_init(gpt_params * params) {
    llama_backend_init();
    llama_numa_init(params->numa);

    llama_model_params model_params = llama_model_params_from_gpt_params(*params);

    llama_model * model = llama_load_model_from_file(params->model.c_str(), model_params);
    if (model == NULL) {
        LOG_ERR("%s: unable to load model\n" , __func__);
        return NULL;
    }
    return model;
}
```
After that we have the following check which is checking to see if the prompt
contains an embedded base64 encoded image (I think, I need to look into this
more and try it out) which is not the case for this session, so else block will
be executed:
```c++
    if (prompt_contains_image(params.prompt)) {
        auto * ctx_llava = llava_init_context(&params, model);

        auto * image_embed = load_image(ctx_llava, &params, "");

        // process the prompt
        process_prompt(ctx_llava, image_embed, &params, params.prompt);

        llama_perf_context_print(ctx_llava->ctx_llama);
        llava_image_embed_free(image_embed);
        ctx_llava->model = NULL;
        llava_free(ctx_llava);
    } else {
        for (auto & image : params.image) {
            auto * ctx_llava = llava_init_context(&params, model);

            auto * image_embed = load_image(ctx_llava, &params, image);
            if (!image_embed) {
                LOG_ERR("%s: failed to load image %s. Terminating\n\n", __func__, image.c_str());
                return 1;
            }

            // process the prompt
            process_prompt(ctx_llava, image_embed, &params, params.prompt);

            llama_perf_context_print(ctx_llava->ctx_llama);
            llava_image_embed_free(image_embed);
            ctx_llava->model = NULL;
            llava_free(ctx_llava);
        }
    }
```
So we can see that `params.image` is in fact a vector so we can supply multiple
images on the command line:
```console
(gdb) ptype params.image
type = std::vector<std::string>
```
Next a `llava_context` will be initialized.
```c++
static struct llava_context * llava_init_context(gpt_params * params, llama_model * model) {
    const char * clip_path = params->mmproj.c_str();

    auto prompt = params->prompt;
    if (prompt.empty()) {
        prompt = "describe the image in detail.";
    }

    auto ctx_clip = clip_model_load(clip_path, /*verbosity=*/ 1);


    llama_context_params ctx_params = llama_context_params_from_gpt_params(*params);
    ctx_params.n_ctx           = params->n_ctx < 2048 ? 2048 : params->n_ctx; // we need a longer context size to process image embeddings

    llama_context * ctx_llama = llama_new_context_with_model(model, ctx_params);

    if (ctx_llama == NULL) {
        LOG_ERR("%s: failed to create the llama_context\n" , __func__);
        return NULL;
    }

    auto * ctx_llava = (struct llava_context *)malloc(sizeof(llava_context));

    ctx_llava->ctx_llama = ctx_llama;
    ctx_llava->ctx_clip = ctx_clip;
    ctx_llava->model = model;
    return ctx_llava;
}
```
Notice that we did not specify a prompt on the command line so the default prompt
will be used which is "describe the image in detail.".
Next `clip_model_load` will be called which is defined in clip.cpp:
```c++
struct clip_ctx * clip_model_load(const char * fname, const int verbosity = 1) {
    struct ggml_context * meta = NULL;

    struct gguf_init_params params = {
        /*.no_alloc = */ true,
        /*.ctx      = */ &meta,
    };
```
So first a `ggml_context` pointer is created named `meta` and used to initialize
`gguf_init_params` which is then passed to `gguf_init_from_file`:
```
    struct gguf_context * ctx = gguf_init_from_file(fname, params);
```
```console
(gdb) p *ctx
$22 = {header = {magic = "GGUF", version = 2, n_tensors = 378, n_kv = 25},
kv = 0x555556b33520, infos = 0x555555b602b0, alignment = 32, offset = 22112,
size = 624429056, data = 0x0}
```
This image encoder (`image encoder for LLaVA`) has 378 tensors which will be
iterated over:
```console
        std::map<enum ggml_type, uint32_t> n_type;

        for (int i = 0; i < n_tensors; i++) {
            enum ggml_type type = gguf_get_tensor_type(ctx, i);

            n_type[type]++;
        }
```
```c
enum ggml_type gguf_get_tensor_type(const struct gguf_context * ctx, int i) {
    return ctx->infos[i].type;
}
```
And infos is of type `struct gguf_tensor_info`:
```console
(gdb) ptype *ctx->infos
type = struct gguf_tensor_info {
    gguf_str name;
    uint32_t n_dims;
    uint64_t ne[4];
    ggml_type type;
    uint64_t offset;
    const void *data;
    size_t size;
}

(gdb) p ctx->infos[0]
$28 = {name = {n = 9, data = 0x555555b4afe0 "mm.0.bias"}, n_dims = 1,
ne = {4096, 1, 1, 1}, type = GGML_TYPE_F32, offset = 0, data = 0x0, size = 0}
```
The above for loop is only used to count the number of tensors of each type.
```console
(gdb) until 1080
```
Actuall this whole block is just for logging so lets skip it but I'll show the
output here:
```console
(gdb) until 1109
clip_model_load: - kv   0:                       general.architecture str              = clip
clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
clip_model_load: - kv   4:                          general.file_type u32              = 1
clip_model_load: - kv   5:                               general.name str              = vit-large336-custom
clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA
clip_model_load: - kv   7:                        clip.projector_type str              = mlp
clip_model_load: - kv   8:                     clip.vision.image_size u32              = 336
clip_model_load: - kv   9:                     clip.vision.patch_size u32              = 14
clip_model_load: - kv  10:               clip.vision.embedding_length u32              = 1024
clip_model_load: - kv  11:            clip.vision.feed_forward_length u32              = 4096
clip_model_load: - kv  12:                 clip.vision.projection_dim u32              = 768
clip_model_load: - kv  13:           clip.vision.attention.head_count u32              = 16
clip_model_load: - kv  14:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010
clip_model_load: - kv  15:                    clip.vision.block_count u32              = 23
clip_model_load: - kv  16:           clip.vision.image_grid_pinpoints arr[i32,10]      = [336, 672, 672, 336, 672, 672, 1008, ...
clip_model_load: - kv  17:          clip.vision.image_crop_resolution u32              = 224
clip_model_load: - kv  18:             clip.vision.image_aspect_ratio str              = anyres
clip_model_load: - kv  19:         clip.vision.image_split_resolution u32              = 224
clip_model_load: - kv  20:            clip.vision.mm_patch_merge_type str              = spatial_unpad
clip_model_load: - kv  21:              clip.vision.mm_projector_type str              = mlp2x_gelu
clip_model_load: - kv  22:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]
clip_model_load: - kv  23:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]
clip_model_load: - kv  24:                              clip.use_gelu bool             = false
clip_model_load: - type  f32:  236 tensors
clip_model_load: - type  f16:  142 tensors
clip_model_load (fname=0x555555b548e0 "models/mmproj-vicuna7b-f16.gguf", verbosity=1)
    at /home/danbev/work/ai/llama.cpp/examples/llava/clip.cpp:1109
```

```c++
    clip_ctx * new_clip = new clip_ctx{};
```
The `clip_ctx` struct is defined in clip.cpp:
```c++
struct clip_ctx {
    bool has_text_encoder    = false;
    bool has_vision_encoder  = false;
    bool has_llava_projector = false;
    bool has_minicpmv_projector = false;
    int minicpmv_version = 2;

    struct clip_vision_model vision_model;
    projector_type proj_type = PROJECTOR_TYPE_MLP;

    float image_mean[3];
    float image_std[3];
    bool use_gelu = false;
    int32_t ftype = 1;

    bool has_class_embedding = true;
    bool has_pre_norm = true;
    bool has_post_norm = false;
    bool has_patch_bias = false;

    struct gguf_context * ctx_gguf;
    struct ggml_context * ctx_data;

    std::vector<uint8_t> buf_compute_meta;

    // memory buffers to evaluate the model
    ggml_backend_buffer_t params_buffer  = NULL;

    ggml_backend_t backend       = NULL;
    ggml_gallocr_t compute_alloc = NULL;

    struct clip_image_size * load_image_size;
};
```
Next the type of the projector will will be read from the model inforation
```c++
            const std::string proj_type = gguf_get_val_str(ctx, idx);
            new_clip->proj_type = clip_projector_type_from_string(proj_type);
```
```console
(gdb) p proj_type
$2 = "mlp
```
The following projector types are currently defined:
```c++
static std::map<projector_type, std::string> PROJECTOR_TYPE_NAMES = {
    { PROJECTOR_TYPE_MLP, "mlp" },
    { PROJECTOR_TYPE_LDP, "ldp" },
    { PROJECTOR_TYPE_LDPV2, "ldpv2"},
    { PROJECTOR_TYPE_RESAMPLER, "resampler"},
};
```
So these different types of approches to mappingg the image embeddings into a
space that can be processed along side text token embeddings.
* MPL
is a multi-layer perceptron which is a feedforward neural network which
does the transformation the image embeddings from CLIP's image encoder into a 
format compatible with the language model.
* LDP TODO: what is this? There is a section above but it needs more info.
* LDPV2 TODO: what is this?
* RESAMPLER TODO: what is this?

So the above will set the projector type on the clip context.
Specific to the MLP type projector is the following:
```c++
        if (new_clip->proj_type == PROJECTOR_TYPE_MLP) {
            if (gguf_find_tensor(ctx, format(TN_LLAVA_PROJ, 3, "weight").c_str()) != -1) {
                new_clip->proj_type = PROJECTOR_TYPE_MLP_NORM;
            }
        }
```
In our case this will not be true:
```console
(gdb) p gguf_find_tensor(ctx, format("mm.%d.%s", 3, "weight"))
$3 = -1
```
Following that we have the initalization of the CUDA backend:
```
#ifdef GGML_USE_CUDA
    new_clip->backend = ggml_backend_cuda_init(0);
    LOG_INF("%s: CLIP using CUDA backend\n", __func__);
#endif
```

After that we have a block which populates the clip context:
```
        int idx = get_key_idx(ctx, KEY_HAS_TEXT_ENC);
        new_clip->has_text_encoder = gguf_get_val_bool(ctx, idx);
        ...
```
This block is setting some of the model values that we saw in the output above.
```
clip_model_load: - kv   0:                       general.architecture str              = clip
clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false
clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true
clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true
```

After that we have the loading of tensors:
```c++
    // load tensors
    {
        std::vector<uint8_t> read_buf;
        struct ggml_init_params params = {
            /*.mem_size =*/ (n_tensors + 1) * ggml_tensor_overhead(),
            /*.mem_buffer =*/ NULL,
            /*.no_alloc =*/ true,
        };
        new_clip->ctx_data = ggml_init(params);
```
Notice where that `no_alloc` is set to true which means that data for the tensor
will not be allocated. See [ggml.md](./ggml.md#no_alloc) for more details on
this.
The we open a input files stream to `models/mmproj-vicuna7b-f16.gguf`:
```c++
        auto fin = std::ifstream(fname, std::ios::binary);
```
And then we will iterate over all the tensors, keep in mind that `ctx` in this
case is a `gguf_context` and not a `ggml_context`, and `meta` is a
`ggml_context`:
```c++
        // add tensors to context
        for (int i = 0; i < n_tensors; ++i) {
            const char * name = gguf_get_tensor_name(ctx, i);
            struct ggml_tensor * t = ggml_get_tensor(meta, name);
            struct ggml_tensor * cur = ggml_dup_tensor(new_clip->ctx_data, t);
            ggml_set_name(cur, name);
        }
```
So we have 378 tensors for this model. Lets go through one of them:
```console
(gdb) p name
$11 = 0x555555b4afe0 "mm.0.bias"
(gdb) p ctx->infos[i]
$12 = {name = {n = 9, data = 0x555555b4afe0 "mm.0.bias"}, n_dims = 1, ne = {4096, 1, 1, 1}, type = GGML_TYPE_F32, offset = 0,
  data = 0x0, size = 0}
(gdb) p ctx->infos[i].name
$13 = {n = 9, data = 0x555555b4afe0 "mm.0.bias"}
(gdb) p ctx->infos[i].name.data
$14 = 0x555555b4afe0 "mm.0.bias"
```
Then the tensor will be looked up in the `ggml_context` meta:
```console
gdb) p *t
$15 = {type = GGML_TYPE_F32, backend = GGML_BACKEND_TYPE_CPU, buffer = 0x0, ne = {4096, 1, 1, 1}, nb = {4, 16384, 16384, 16384}, 
op = GGML_OP_NONE, op_params = {0 <repeats 16 times>}, flags = 0, grad = 0x0, src = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, view_src = 0x0, view_offs = 0, data = 0x0, name = "mm.0.bias", '\000' <repeats 54 times>, extra = 0x0}
```
And this will be duplicated into a different `ggml_context` which is the field
of the `clip_ctx` struct. The name of a tensor is not duplicated so we need to
set the the name of the new tensor.

After all the tensors have been copied into the `new_clip` `ggml_context`
(`ctx_data`) these tensors will be allocated in the backend which is:
```console
(gdb) p new_clip->backend.iface.get_name(new_clip->backend)
$21 = 0x555556b328a8 "CUDA0"
```
And the allocation is done like this:
```c++
        new_clip->params_buffer = ggml_backend_alloc_ctx_tensors(new_clip->ctx_data, new_clip->backend);
```
```c++
ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors(struct ggml_context * ctx, ggml_backend_t backend) {
    return ggml_backend_alloc_ctx_tensors_from_buft(ctx, ggml_backend_get_default_buffer_type(backend));
}
```
Note that this function suffix is `buft` which stands for buffer type. This will
end up in ggml-alloc.c:
```c
ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors_from_buft(struct ggml_context * ctx,
    ggml_backend_buffer_type_t buft) {
    GGML_ASSERT(ggml_get_no_alloc(ctx) == true);
```
And notice here that the assert is checking that `no_alloc` is set to true which
makes sense as the data for the tensors are to be allocated in the backend.

This will get the first tensor from the `new_clip->ctx_data` ggml context:
```c
    struct ggml_tensor * first = ggml_get_first_tensor(ctx);
```
And then start from that tensor and iterate over all tensors to calculate the
size of the buffer required to store all the tensors:
```c
    for (struct ggml_tensor * t = first; t != NULL; t = ggml_get_next_tensor(ctx, t)) {
        size_t this_size = 0;
        if (t->data == NULL && t->view_src == NULL) {
            this_size = GGML_PAD(ggml_backend_buft_get_alloc_size(buft, t), alignment);
        }

        if ((cur_buf_size + this_size) > max_size) {
            // allocate tensors in the current buffer
            if (!alloc_tensor_range(ctx, first, t, buft, cur_buf_size, &buffers, &n_buffers)) {
                return NULL;
            }
            first = t;
            cur_buf_size = this_size;
        } else {
            cur_buf_size += this_size;
        }
    }
```
Notice that this will iterate over the tensors and calculate the size of the
buffer required to store all the tensors. If the size of the buffer is greater
than the maximum size then then tensors up to the current tensor will be
allocated on the CUDA device by calling `alloc_tensor_range`.
```c
    // allocate remaining tensors
    if (cur_buf_size > 0) {
        if (!alloc_tensor_range(ctx, first, NULL, buft, cur_buf_size, &buffers, &n_buffers)) {
            return NULL;
        }
    }
```
The first thing that happens where is that a buffer will be allocated using
the backend type:
```c
static bool alloc_tensor_range(struct ggml_context * ctx,
        struct ggml_tensor * first, struct ggml_tensor * last,
        ggml_backend_buffer_type_t buft, size_t size,
        ggml_backend_buffer_t ** buffers, size_t * n_buffers) {
    ggml_backend_buffer_t buffer = ggml_backend_buft_alloc_buffer(buft, size);
    if (buffer == NULL) {
        ...
        free(*buffers);
        return false;
    }

    struct ggml_tallocr tallocr = ggml_tallocr_new(buffer);

    for (struct ggml_tensor * t = first; t != last; t = ggml_get_next_tensor(ctx, t)) {
        if (t->data == NULL) {
            if (t->view_src == NULL) {
                ggml_tallocr_alloc(&tallocr, t);
            } else if (t->buffer == NULL) {
                ggml_backend_view_init(t);
            }
        } else {
            if (t->view_src != NULL && t->buffer == NULL) {
                // view of a pre-allocated tensor
                ggml_backend_view_init(t);
            }
        }
    }

    *buffers = realloc(*buffers, sizeof(ggml_backend_buffer_t) * (*n_buffers + 1));
    (*buffers)[(*n_buffers)++] = buffer;

    return true;
}
```
So the `ggml_backend_buft_alloc_buffer` will end up in `ggml-cuda.cu` where we
first set the device to be used:
```c
GGML_CALL static ggml_backend_buffer_t ggml_backend_cuda_buffer_type_alloc_buffer(ggml_backend_buffer_type_t buft, size_t size) {
    ggml_backend_cuda_buffer_type_context * buft_ctx = (ggml_backend_cuda_buffer_type_context *)buft->context;

    ggml_cuda_set_device(buft_ctx->device);

    size = std::max(size, (size_t)1); // cudaMalloc returns null for size 0

    void * dev_ptr;
    cudaError_t err = ggml_cuda_device_malloc(&dev_ptr, size, buft_ctx->device);
    if (err != cudaSuccess) {
        ...
        return nullptr;
    }

    ggml_backend_cuda_buffer_context * ctx = new ggml_backend_cuda_buffer_context(buft_ctx->device, dev_ptr);

    return ggml_backend_buffer_init(buft, ggml_backend_cuda_buffer_interface, ctx, size);
}
```
And we can see that a new `ggml_backend_cuda_buffer_context` is created:
```console
(gdb) p *ctx
$35 = {device = 0, dev_ptr = 0x7ffdc6000000, name = "CUDA0"}
```
And notice that the `dev_ptr` is a pointer to the device memory, so this is
memory on the CUDA device.

And finally `ggml_backend_buffer_init` is called to initialize the buffer:
```c
GGML_CALL ggml_backend_buffer_t ggml_backend_buffer_init(
               ggml_backend_buffer_type_t      buft,
        struct ggml_backend_buffer_i           iface,
               ggml_backend_buffer_context_t   context,
               size_t                          size) {
    ggml_backend_buffer_t buffer = malloc(sizeof(struct ggml_backend_buffer));

    (*buffer) = (struct ggml_backend_buffer) {
        /* .interface = */ iface,
        /* .buft      = */ buft,
        /* .context   = */ context,
        /* .size      = */ size,
        /* .usage     = */ GGML_BACKEND_BUFFER_USAGE_ANY
    };

    return buffer;
}
```
Now, the `ggml_backend_buffer` is a allocated on the host, but the context has
its `dev_ptr` pointing to memory on the CUDA device.

This will be returned to `alloc_tensor_range` where we will call
`ggml_tallocr_new` where talloc stands for tensor allocator:
```
    struct ggml_tallocr tallocr = ggml_tallocr_new(buffer);
```
```c
// Tensor allocator
struct ggml_tallocr {
    ggml_backend_buffer_t buffer;
    void * base;
    size_t alignment;
    size_t offset;
};
```
After that all the tensors will be iterated over. Recall that we set `no_alloc`
to true to the tensors in this case will not have an data pointer and their
`view_src` will also be NULL:
```
    for (struct ggml_tensor * t = first; t != last; t = ggml_get_next_tensor(ctx, t)) {
        if (t->data == NULL) {
            if (t->view_src == NULL) {
                ggml_tallocr_alloc(&tallocr, t);
            } else if (t->buffer == NULL) {
                ggml_backend_view_init(t);
            }
        } else {
            if (t->view_src != NULL && t->buffer == NULL) {
                // view of a pre-allocated tensor
                ggml_backend_view_init(t);
            }
        }
```
So `ggml_tallocr_alloc` will be called for each tensor. Lets take a look at the
first tensor before this call and then we can inspect it again afterwards:
```console
(gdb) p *tensor
$45 = {type = GGML_TYPE_F32, backend = GGML_BACKEND_TYPE_CPU, buffer = 0x0,
ne = {4096, 1, 1, 1}, nb = {4, 16384, 16384, 16384}, op = GGML_OP_NONE,
op_params = {0 <repeats 16 times>}, flags = 0, grad = 0x0, src = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
0x0}, view_src = 0x0, view_offs = 0, data = 0x0, name = "mm.0.bias", '\000' <repeats 54 times>, extra = 0x0}
```

```c
void ggml_tallocr_alloc(struct ggml_tallocr * talloc, struct ggml_tensor * tensor) {
    size_t size = ggml_backend_buffer_get_alloc_size(talloc->buffer, tensor);
    size = GGML_PAD(size, talloc->alignment);

    if (talloc->offset + size > ggml_backend_buffer_get_size(talloc->buffer)) {
        fprintf(stderr, "%s: not enough space in the buffer to allocate %s (needed %zu, available %zu)\n",
                __func__, tensor->name, size, ggml_backend_buffer_get_size(talloc->buffer) - talloc->offset);
        GGML_ABORT("not enough space in the buffer");
    }

    void * addr = (char *)ggml_backend_buffer_get_base(talloc->buffer) + talloc->offset;
    talloc->offset += size;

    assert(((uintptr_t)addr % talloc->alignment) == 0);

    ggml_backend_tensor_alloc(talloc->buffer, tensor, addr);
}
```
In our case size will be 16384 bytes.
```c
void ggml_backend_tensor_alloc(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor, void * addr) {
    // removed asserts for readability
    ...
    tensor->buffer = buffer;
    tensor->data = addr;
    ggml_backend_buffer_init_tensor(buffer, tensor);
}
```
Notice that this is updating the buffer field and the data field of the tensor.

And `ggml_backend_buffer_init_tensor`
```c
GGML_CALL void ggml_backend_buffer_init_tensor(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor) {
    // init_tensor is optional
    if (buffer->iface.init_tensor) {
        buffer->iface.init_tensor(buffer, tensor);
    }
}
```
This will land in `ggml-cuda.cu`:
```c
GGML_CALL static void ggml_backend_cuda_buffer_init_tensor(ggml_backend_buffer_t buffer, ggml_tensor * tensor) {
    ggml_backend_cuda_buffer_context * ctx = (ggml_backend_cuda_buffer_context *)buffer->context;
    ...

    if (ggml_is_quantized(tensor->type) && tensor->view_src == nullptr &&
        ggml_backend_buffer_get_usage(buffer) != GGML_BACKEND_BUFFER_USAGE_COMPUTE) {
        // initialize padding to 0 to avoid possible NaN values
        size_t original_size = ggml_nbytes(tensor);
        size_t padded_size = ggml_backend_buft_get_alloc_size(buffer->buft, tensor);

        if (padded_size > original_size) {
            ggml_cuda_set_device(ctx->device);
            CUDA_CHECK(cudaMemset((char *)tensor->data + original_size, 0, padded_size - original_size));
        }
    }
}
```
In our case the above will do nothing as the tensor is not quantized.
And all of the tensors will go through the same process where they will have
their buffer and data fields updated. So after this the tensors data pointer
will point to the memory of the CUDA device.

So to recap, we have been looking into `alloc_tensor_range` which looks like
this:
```c
static bool alloc_tensor_range(struct ggml_context * ctx,
        struct ggml_tensor * first, struct ggml_tensor * last,
        ggml_backend_buffer_type_t buft, size_t size,
        ggml_backend_buffer_t ** buffers, size_t * n_buffers) {
    ggml_backend_buffer_t buffer = ggml_backend_buft_alloc_buffer(buft, size);
    if (buffer == NULL) {
        for (size_t i = 0; i < *n_buffers; i++) {
            ggml_backend_buffer_free((*buffers)[i]);
        }
        free(*buffers);
        return false;
    }

    struct ggml_tallocr tallocr = ggml_tallocr_new(buffer);

    for (struct ggml_tensor * t = first; t != last; t = ggml_get_next_tensor(ctx, t)) {
        if (t->data == NULL) {
            if (t->view_src == NULL) {
                ggml_tallocr_alloc(&tallocr, t);
            } else if (t->buffer == NULL) {
                ggml_backend_view_init(t);
            }
        } else {
            if (t->view_src != NULL && t->buffer == NULL) {
                // view of a pre-allocated tensor
                ggml_backend_view_init(t);
            }
        }
    }

    *buffers = realloc(*buffers, sizeof(ggml_backend_buffer_t) * (*n_buffers + 1));
    (*buffers)[(*n_buffers)++] = buffer;

    return true;
}
```
And we have seen most of this apart from the last three lines. Recall that this
function was called when `max_size` was reached and the `realloc` is adding a
the new buffer we created at the start of this function. Realloc will behave
like malloc if `*buffers` is null, otherwise if there is not enough space it
will copy the old buffer to a new location and free the old buffer.
```console
(gdb) p *buffers
$55 = (ggml_backend_buffer_t *) 0x0

(gdb) p *buffers
$60 = (ggml_backend_buffer_t *) 0x555556b2fda0
```
So now there will be a space for the new buffer in the array since it was
expanded:
```console
(gdb) p (*buffers)[0]
$68 = (ggml_backend_buffer_t) 0x555003e796ef
```
So we can dereference the pointer to the buffer, and index the array element
using the value that `n_buffers` currently points to (notice the parentheses
which makes this using the value as an index), and set that to the the new
buffer. The value `n_buffers` will then be incremented by one.
```console
(gdb) p *n_buffers
$72 = 0
```
```c
    (*buffers)[(*n_buffers)++] = buffer;
```
```console
(gdb) p *n_buffers
$73 = 1
(gdb) p *(*buffers)[0]
$75 = {iface = {get_name = 0x7fffe1f741c8 <ggml_backend_cuda_buffer_get_name(ggml_backend_buffer_t)>,
    free_buffer = 0x7fffe1f74218 <ggml_backend_cuda_buffer_free_buffer(ggml_backend_buffer_t)>,
    get_base = 0x7fffe1f74255 <ggml_backend_cuda_buffer_get_base(ggml_backend_buffer_t)>,
    init_tensor = 0x7fffe1f74277 <ggml_backend_cuda_buffer_init_tensor(ggml_backend_buffer_t, ggml_tensor*)>,
    memset_tensor = 0x7fffe1f743e6 <ggml_backend_cuda_buffer_memset_tensor(ggml_backend_buffer_t, ggml_tensor*, uint8_t, size_t, size_t)>,
    set_tensor = 0x7fffe1f744d0 <ggml_backend_cuda_buffer_set_tensor(ggml_backend_buffer_t, ggml_tensor*, void const*, size_t, size_t)>,
    get_tensor = 0x7fffe1f745c0 <ggml_backend_cuda_buffer_get_tensor(ggml_backend_buffer_t, ggml_tensor const*, void*, size_t, size_t)>, cpy_tensor = 0x7fffe1f746b0 <ggml_backend_cuda_buffer_cpy_tensor(ggml_backend_buffer_t, ggml_tensor const*, ggml_tensor*)>,
    clear = 0x7fffe1f74860 <ggml_backend_cuda_buffer_clear(ggml_backend_buffer_t, uint8_t)>, reset = 0x0},
  buft = 0x7ffff781a7e0 <ggml_backend_cuda_buffer_type::ggml_backend_cuda_buffer_types>, context = 0x555555b499e0, size = 624429056,
  usage = GGML_BACKEND_BUFFER_USAGE_ANY}
```
That will return us back in `ggml_backend_alloc_ctx_tensors_from_buft`:
```c
    ggml_backend_buffer_t buffer;
    if (n_buffers == 1) {
        buffer = buffers[0];
    } else {
        buffer = ggml_backend_multi_buffer_alloc_buffer(buffers, n_buffers);
    }
    free(buffers);
    return buffer;
```
And that will return us back to `clip_model_load`. Now the data for the tensors
have been alloced on the backend but they don't contain any data yet.
```c++
        // alloc memory and offload data
        new_clip->params_buffer = ggml_backend_alloc_ctx_tensors(new_clip->ctx_data, new_clip->backend);
        for (int i = 0; i < n_tensors; ++i) {
            const char * name = gguf_get_tensor_name(ctx, i);
            struct ggml_tensor * cur = ggml_get_tensor(new_clip->ctx_data, name);
            const size_t offset = gguf_get_data_offset(ctx) + gguf_get_tensor_offset(ctx, i);
            fin.seekg(offset, std::ios::beg);
            ...

            int num_bytes = ggml_nbytes(cur);
            if (ggml_backend_buffer_is_host(new_clip->params_buffer)) {
                // for the CPU and Metal backend, we can read directly into the tensor
                fin.read(reinterpret_cast<char *>(cur->data), num_bytes);
            } else {
                // read into a temporary buffer first, then copy to device memory
                read_buf.resize(num_bytes);
                fin.read(reinterpret_cast<char *>(read_buf.data()), num_bytes);
                ggml_backend_tensor_set(cur, read_buf.data(), 0, num_bytes);
            }
        }
```
In our case the else block will be executed as the backend is CUDA. Lets take a
closer look at `ggml_backend_tensor_set`:
```c
GGML_CALL void ggml_backend_tensor_set(struct ggml_tensor * tensor, const void * data, size_t offset, size_t size) {
    ggml_backend_buffer_t buf = tensor->view_src ? tensor->view_src->buffer : tensor->buffer;
    ...
    buf->iface.set_tensor(buf, tensor, data, offset, size);
}
```
```c
GGML_CALL static void ggml_backend_cuda_buffer_set_tensor(
    ggml_backend_buffer_t buffer,
    ggml_tensor * tensor,
    const void * data,
    size_t offset,
    size_t size) {
    ggml_backend_cuda_buffer_context * ctx = (ggml_backend_cuda_buffer_context *)buffer->context;

    ggml_cuda_set_device(ctx->device);
    CUDA_CHECK(cudaMemcpyAsync((char *)tensor->data + offset, data, size, cudaMemcpyHostToDevice, cudaStreamPerThread));
    CUDA_CHECK(cudaStreamSynchronize(cudaStreamPerThread));
}
```
This is copying data from the host, which is the tensor data that was read from
the the file into a buffer and then passed to this function. Notice that the
destionation is the tensor data pointer which is on the CUDA device. 
And this is done for all the tensors.

Next we have the vision model loading:
```c++
    // vision model
    if (new_clip->has_vision_encoder) {
        // load vision model
        auto & vision_model = new_clip->vision_model;
        auto & hparams = vision_model.hparams;
        hparams.hidden_size    = get_u32(ctx, format(KEY_N_EMBD, "vision"));
        hparams.n_head         = get_u32(ctx, format(KEY_N_HEAD, "vision"));
        hparams.n_intermediate = get_u32(ctx, format(KEY_N_FF, "vision"));
        hparams.n_layer        = get_u32(ctx, format(KEY_N_BLOCK, "vision"));
        hparams.image_size     = get_u32(ctx, KEY_IMAGE_SIZE);
        hparams.patch_size     = get_u32(ctx, KEY_PATCH_SIZE);
        hparams.projection_dim = get_u32(ctx, format(KEY_PROJ_DIM, "vision"));
        hparams.eps            = get_f32(ctx, format(KEY_LAYER_NORM_EPS, "vision"));
```
The above is getting values for the keys in the model:
```console
$ ./inspect-model.sh models/mmproj-vicuna7b-f16.gguf
INFO:gguf-dump:* Loading: models/mmproj-vicuna7b-f16.gguf
* File is LITTLE endian, script is running on a LITTLE endian host.
* Dumping 28 key/value pair(s)
     ...
     13: UINT32     |        1 | clip.vision.patch_size = 14
     14: UINT32     |        1 | clip.vision.embedding_length = 1024
     15: UINT32     |        1 | clip.vision.feed_forward_length = 4096
     16: UINT32     |        1 | clip.vision.projection_dim = 768
     17: UINT32     |        1 | clip.vision.attention.head_count = 16
     18: FLOAT32    |        1 | clip.vision.attention.layer_norm_epsilon = 9.999999747378752e-06
```
Following that we have:
```c
        try {
            int idx = get_key_idx(ctx, KEY_IMAGE_GRID_PINPOINTS);
            int n = gguf_get_arr_n(ctx, idx);
            const int32_t * pinpoints = (const int32_t *)gguf_get_arr_data(ctx, idx);
            for (int i = 0; i < 32 && i < n && pinpoints[i] != 0; ++i) {
                hparams.image_grid_pinpoints[i] = pinpoints[i];
            }
            if (n < 32)
                hparams.image_grid_pinpoints[n] = 0;
        } catch (std::runtime_error & /*e*/) {
            hparams.image_grid_pinpoints[0]=0;
        }
```
Now, we we inspect the output of the model we can see the following:
```
     20: [INT32]    |       10 | clip.vision.image_grid_pinpoints
 ```
 This is an array of 10 integers representing something called pin points.
 A vision model will often divide an image into a grid of cells/patches. The grid
 enables a systemactic way of processing the image.
```console
pinpoints[0] = 336
pinpoints[1] = 672
pinpoints[2] = 672
pinpoints[3] = 672
pinpoints[4] = 672
pinpoints[5] = 1008
pinpoints[6] = 336
pinpoints[7] = 336
pinpoints[8] = 336
pinpoints[9] = 1008
```

clip.vision.image_size = 336
clip.vision.patch_size = 14
clip.vision.image_grid_pinpoints = [336, 672, 672, 336, 672, 672, 1008, 336, 336, 1008]
```
So we have an image size of `336x336` pixels, and a patch size of `14x14`
pixels. So if we divide 336 by 14 we get 24 which means that the image will be
divided into a grid of 24x24 patches.
```
  +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
0 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |10 |11 |12 |13 |14 |15 |16 |17 |18 |19 |20 |21 |22 |23 |
  +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
1 |24 |25 |26 |...                                                                         ...|47 |
  +---+---+---+-------------------------------------------------------------------------------+---+
  |...|                                                                                       |...|
  +---+---------------------------------------------------------------------------------------+---+
23|552|553|554|...                                                                         ...|575|
  +---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+

Each box is 14x14 pixels
Each row is 24 boxes
And we have 24x24=576 boxes in total
576 * 14 * 14 = 
576 * 196     = 112896
336 * 336     = 112896
```
The values in the pinpoints array are coordinates in the image grid that the
model will focus more on (I think).
```
(336, 672)
(672, 672)
(672, 1008)
(336, 336)
(336, 1008)
```
We can think of these as regions/tiles in the patch.
```console
0
+-------------------------------------------------------------> x
| [ (0,0)      ] [ (336,0)    ] [ (672,0)    ] [ (1008,0)   ]
| [            ] [            ] [            ] [            ]
| [------------] [------------] [------------] [------------]
| [ (0,336)    ] [ (336,336)  ] [ (672,336)  ] [ (1008,336) ]
| [            ] [            ] [            ] [            ]
| [------------] [------------] [------------] [------------]
| [ (0,672)    ] [ (336,672)  ] [ (672,672)  ] [ (1008,672) ]
| [            ] [            ] [            ] [            ]
| [------------] [------------] [------------] [------------]
| [ (0,1008)   ] [ (336,1008) ] [ (672,1008) ] [ (1008,1008)]
| [            ] [            ] [            ] [            ]
↓
y

0
+-------------------------------------------------------------> x
| [ (0,0)      ] [ (336,0)    ] [ (672,0)    ] [ (1008,0)   ]
| [            ] [            ] [            ] [            ]
| [------------] [------------] [------------] [------------]
| [ (0,336)    ] [XXXXXXXXXXXX] [ (672,336)  ] [ (1008,336) ]
| [            ] [XXXXXXXXXXXX] [            ] [            ]
| [------------] [------------] [------------] [------------]
| [ (0,672)    ] [XXXXXXXXXXXX] [XXXXXXXXXXXX] [ (1008,672) ]
| [            ] [XXXXXXXXXXXX] [XXXXXXXXXXXX] [            ]
| [------------] [------------] [------------] [------------]
| [ (0,1008)   ] [XXXXXXXXXXXX] [XXXXXXXXXXXX] [ (1008,1008)]
| [            ] [XXXXXXXXXXXX] [XXXXXXXXXXXX] [            ]
↓
y
```
This could be a way to focus on specific regions of the image and we can see
that the center of the image is focused on more than the edges and this makes
sense I guess as most subjects of images are in the center.

Next we have the merge type:
```
        try {
            int idx = get_key_idx(ctx, KEY_MM_PATCH_MERGE_TYPE);
            strcpy(hparams.mm_patch_merge_type, gguf_get_val_str(ctx, idx));
        } catch (std::runtime_error & /*e*/) {
            strcpy(hparams.mm_patch_merge_type, "flat");
        }
```
We can see that the default type is `flat` which I believe means that that a
grid of 3x3 patches will be flattened into a single vector:
```
Sequence = [Patch at (336, 672), Patch at (672, 336), Patch at (672, 672), Patch at (1008, 336), Patch at (336, 1008)]
```
The type that used in this model is:
```console
(gdb) p gguf_get_val_str(ctx, idx)
$119 = 0x555555b4b240 "spatial_unpad"
```
So what would it look like:
```console

Input Sequence (flattened grid with empty slots):
[0, 0, 0, 0, 0, P(672,336), P(1008,336), 0, P(336,672), P(672,672), 0, 0, P(336,1008), 0, 0, 0]

Possible mask:
[0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0]
```
After that we have:
```c++
        try {
            hparams.image_crop_resolution = get_u32(ctx, KEY_IMAGE_CROP_RESOLUTION); // llava-1.6
        } catch(const std::exception& /*e*/) {
            hparams.image_crop_resolution = hparams.image_size;
        }
```
This is about the resolution in pixels to which images are to be cropped during
pre-processing before being fed into the model. So if we input an image it will
be cropped. Notice that this is first copping to 224 and then resizing to 336
which sounded a little strange to me. But reading some more about this it seems
like this may be because of the way the model was trained. If the model was
trained on 224x224 images (like ResNet) then having the same value here might
ensure compability and optimal performance.

* Load image using its original dimensions.
* Crop to 224x224 (center crop?)
* Resize to 336x336
* Normalize (mean and std)

Next we have the mean and std used for the normalization:
```c
        int idx_mean = get_key_idx(ctx, KEY_IMAGE_MEAN);
        int idx_std  = get_key_idx(ctx, KEY_IMAGE_STD);
```
These are also arrays:
```console
     26: [FLOAT32]  |        3 | clip.vision.image_mean
     27: [FLOAT32]  |        3 | clip.vision.image_std
```
```c++
        for (int i = 0; i < 3; ++i) {
            new_clip->image_mean[i] = mean_data[i];
            new_clip->image_std[i]  = std_data[i];
        }
```
```console
(gdb) p new_clip->image_mean[0]
$1 = 0.48145467
(gdb) p new_clip->image_mean[1]
$2 = 0.457827508
(gdb) p new_clip->image_mean[2]
$3 = 0.408210725
(gdb) p new_clip->image_std[0]
$4 = 0.268629551
(gdb) p new_clip->image_std[1]
$5 = 0.26130259
(gdb) p new_clip->image_std[2]
$6 = 0.275777102
```
I think the intuition here is that since we have rescaled the image we want to
adjust the pixel values to match what the model was trained on.

Next, we have a number of tensor that will be set on the vision model
```c++
        try {
            vision_model.class_embedding  = get_tensor(new_clip->ctx_data, TN_CLASS_EMBD);
            new_clip->has_class_embedding = true;
        } catch (const std::exception& /*e*/) {
            new_clip->has_class_embedding = false;
        }
```
This is the class token which aggragates information from all patches and is
used for classification.
```console
(gdb) p *vision_model.class_embedding
$8 = {type = GGML_TYPE_F32, backend = GGML_BACKEND_TYPE_CPU, buffer = 0x555555cc12e0, ne = {1024, 1, 1, 1}, nb = {4, 4096, 4096,
    4096}, op = GGML_OP_NONE, op_params = {0 <repeats 16 times>}, flags = 0, grad = 0x0, src = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
    0x0, 0x0, 0x0}, view_src = 0x0, view_offs = 0, data = 0x7ffdc880c000, name = "v.class_embd", '\000' <repeats 51 times>,
  extra = 0x0}
```
After that we have the weights and bias for the pre-layer normalization (this is
applied before the transformer blocks):
```c++
        try {
            vision_model.pre_ln_w  = get_tensor(new_clip->ctx_data, format(TN_LN_PRE, "v", "weight"));
            vision_model.pre_ln_b  = get_tensor(new_clip->ctx_data, format(TN_LN_PRE, "v", "bias"));
            new_clip->has_pre_norm = true;
        } catch (std::exception & /*e*/) {
            new_clip->has_pre_norm = false;
        }
```
```console
(gdb) p *vision_model.pre_ln_w
$10 = {type = GGML_TYPE_F32, backend = GGML_BACKEND_TYPE_CPU, buffer = 0x555555cc12e0, ne = {1024, 1, 1, 1}, nb = {4, 4096, 4096,
    4096}, op = GGML_OP_NONE, op_params = {0 <repeats 16 times>}, flags = 0, grad = 0x0, src = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
    0x0, 0x0, 0x0}, view_src = 0x0, view_offs = 0, data = 0x7ffdc8a53800, name = "v.pre_ln.weight", '\000' <repeats 48 times>,
  extra = 0x0}
(gdb) p *vision_model.pre_ln_b
$11 = {type = GGML_TYPE_F32, backend = GGML_BACKEND_TYPE_CPU, buffer = 0x555555cc12e0, ne = {1024, 1, 1, 1}, nb = {4, 4096, 4096,
    4096}, op = GGML_OP_NONE, op_params = {0 <repeats 16 times>}, flags = 0, grad = 0x0, src = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
    0x0, 0x0, 0x0}, view_src = 0x0, view_offs = 0, data = 0x7ffdc8a54800, name = "v.pre_ln.bias", '\000' <repeats 50 times>,
  extra = 0x0}
```
Next we have the weights and bias for the post-layer normalization (this is
applied after the transformer blocks):
```c++
        try {
            vision_model.post_ln_w  = get_tensor(new_clip->ctx_data, format(TN_LN_POST, "v", "weight"));
            vision_model.post_ln_b  = get_tensor(new_clip->ctx_data, format(TN_LN_POST, "v", "bias"));
            new_clip->has_post_norm = true;
        } catch (std::exception & /*e*/) {
            new_clip->has_post_norm = false;
        }
```
I'm skipping a head as some of the tensor do not exist in this model. One thing
to note when debugging is that if you are stepping then `get_tensor` may throw
and exception and this will cause the debugger to continue executing. Just
setting a breakpoint outside of the block or in the catch block will allow you
to continue stepping.

Next, we have the patch embeddings whch are used to project the flattened
image patches into the models embedding space, and the positional embedding
which encode the position of each patch in the image grid:
```c++
        try {
            vision_model.patch_embeddings    = get_tensor(new_clip->ctx_data, TN_PATCH_EMBD);
            vision_model.position_embeddings = get_tensor(new_clip->ctx_data, format(TN_POS_EMBD, "v"));
        } catch(const std::exception& /*e*/) {
            LOG_ERR("%s: failed to load vision model tensors\n", __func__);
        }
```
```console
(gdb) p *vision_model.patch_embeddings
$13 = {type = GGML_TYPE_F16, backend = GGML_BACKEND_TYPE_CPU, buffer = 0x555555cc12e0, ne = {14, 14, 3, 1024}, nb = {2, 28, 392,
    1176}, op = GGML_OP_NONE, op_params = {0 <repeats 16 times>}, flags = 0, grad = 0x0, src = {0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0,
    0x0, 0x0, 0x0}, view_src = 0x0, view_offs = 0, data = 0x7ffdc880d000, name = "v.patch_embd.weight", '\000' <repeats 44 times>,
  extra = 0x0}

(gdb) p *vision_model.position_embeddings
$14 = {type = GGML_TYPE_F16, backend = GGML_BACKEND_TYPE_CPU, buffer = 0x555555cc12e0, ne = {1024, 577, 1, 1}, nb = {2, 2048,
    1181696, 1181696}, op = GGML_OP_NONE, op_params = {0 <repeats 16 times>}, flags = 0, grad = 0x0, src = {0x0, 0x0, 0x0, 0x0, 0x0,
    0x0, 0x0, 0x0, 0x0, 0x0}, view_src = 0x0, view_offs = 0, data = 0x7ffdc8933000,
  name = "v.position_embd.weight", '\000' <repeats 41 times>, extra = 0x0}
```

Following that we have:
```c++
        // LLaVA projection
        if (new_clip->proj_type == PROJECTOR_TYPE_MLP || new_clip->proj_type == PROJECTOR_TYPE_MLP_NORM) {
            vision_model.mm_0_w              = get_tensor(new_clip->ctx_data, format(TN_LLAVA_PROJ, 0, "weight"));
            vision_model.mm_0_b              = get_tensor(new_clip->ctx_data, format(TN_LLAVA_PROJ, 0, "bias"));

```
So these are the weights and bias for the first projection linear layer
```console
(gdb) p *vision_model.mm_0_w
$17 = {type = GGML_TYPE_F16, backend = GGML_BACKEND_TYPE_CPU, buffer = 0x555555cc12e0, ne = {1024, 4096, 1, 1}, nb = {2, 2048,
    8388608, 8388608}, op = GGML_OP_NONE, op_params = {0 <repeats 16 times>}, flags = 0, grad = 0x0, src = {0x0, 0x0, 0x0, 0x0, 0x0,
    0x0, 0x0, 0x0, 0x0, 0x0}, view_src = 0x0, view_offs = 0, data = 0x7ffdc6004000, name = "mm.0.weight", '\000' <repeats 52 times>,
  extra = 0x0}
```
Following that there are number of try/catch blocks for loading tensors for
different models type(?) like Yi.

Next the vision models layer is resized to the number of layers in the model
which is 23 in this case:
```c++
        vision_model.layers.resize(hparams.n_layer);
        for (int il = 0; il < hparams.n_layer; ++il) {
            auto & layer = vision_model.layers[il];
```
```console
(gdb) ptype vision_model.layers
type = std::vector<clip_layer>
```
So what does a `clip_layer` look like
```console
(gdb) ptype clip_layer
type = struct clip_layer {
    ggml_tensor *k_w;
    ggml_tensor *k_b;
    ggml_tensor *q_w;
    ggml_tensor *q_b;
    ggml_tensor *v_w;
    ggml_tensor *v_b;
    ggml_tensor *o_w;
    ggml_tensor *o_b;
    ggml_tensor *ln_1_w;
    ggml_tensor *ln_1_b;
    ggml_tensor *ff_i_w;
    ggml_tensor *ff_i_b;
    ggml_tensor *ff_o_w;
    ggml_tensor *ff_o_b;
    ggml_tensor *ln_2_w;
    ggml_tensor *ln_2_b;
}
```
Now this is the text encoder part of the model and represents a a tranformer
attention layer. These layers will get populated clip context.

After that the current `gguf` context is set on the clip context:
```c++
    new_clip->ctx_gguf = ctx;
```
And the last things that happen before returning the clip context is:
```c++
    // measure mem requirement and allocate
    {
        new_clip->buf_compute_meta.resize(GGML_DEFAULT_GRAPH_SIZE * ggml_tensor_overhead() + ggml_graph_overhead());
        new_clip->compute_alloc = ggml_gallocr_new(ggml_backend_get_default_buffer_type(new_clip->backend));
        clip_image_f32_batch batch;
        batch.size = 1;
        ggml_cgraph * gf = clip_image_build_graph(new_clip, &batch, nullptr, false);
        ggml_gallocr_reserve(new_clip->compute_alloc, gf);
        size_t compute_memory_buffer_size = ggml_gallocr_get_buffer_size(new_clip->compute_alloc, 0);
        LOG_INF("%s: compute allocated memory: %.2f MB\n", __func__, compute_memory_buffer_size /1024.0/1024.0);
    }
```

_wip_

```console
(gdb) ptype new_clip->vision_model.hparams
type = struct clip_hparams {
    int32_t image_size;
    int32_t patch_size;
    int32_t hidden_size;
    int32_t n_intermediate;
    int32_t projection_dim;
    int32_t n_head;
    int32_t n_layer;
    float eps;
    char mm_patch_merge_type[32];
    int32_t image_grid_pinpoints[32];
    int32_t image_crop_resolution;
}
```


### `std_image`
The clip implementation in the example (clip.cpp) used `std_image.h` which is
a from the [single-file public domain library](https://github.com/nothings/stb)
which was created by Sean T. Barrett, hence `std`. The `std_image.h` header
contains code related to loading and decoding images from files and memory in
various formats like PNG, JPEG, BMP, PSD, GIF, HDR, PIC.
