## Whisper WASM notes
The whisper examples that use WebAssembly are built using Emscripten. 

### Command example
This is an example of voice command recognition using the microphone.

```console
$ mkdir build-em && cd build-em
$ emcmake cmake ..
configure: cmake .. -DCMAKE_TOOLCHAIN_FILE=/Users/danbev/work/wasm/emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DCMAKE_CROSSCOMPILING_EMULATOR=/Users/danbev/work/wasm/emsdk/node/20.18.0_64bit/bin/node
-- The C compiler identification is Clang 21.0.0
-- The CXX compiler identification is Clang 21.0.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Users/danbev/work/wasm/emsdk/upstream/emscripten/emcc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Users/danbev/work/wasm/emsdk/upstream/emscripten/em++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /opt/homebrew/bin/git (found version "2.46.0")
-- Setting GGML_NATIVE_DEFAULT to OFF
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86
-- Including CPU backend
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES)
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES)
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND)
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- Unknown architecture
-- Adding CPU backend variant ggml-cpu:
-- Embedding WASM inside main.js
-- Embedding WASM inside stream.js
-- Embedding WASM inside command.js
-- Embedding WASM inside bench.js
-- Configuring done (7.6s)
-- Generating done (0.0s)
-- Build files have been written to: /Users/danbev/work/ai/whisper-work/build-em
```
So this sets up the `CMAKE_TOOLCHAIN_FILE` which set up the CMake variables required for Emscripten.
And also notice `DCMAKE_CROSSCOMPILING_EMULATOR=/Users/danbev/work/wasm/emsdk/node/20.18.0_64bit/bin/node`
which is used because Emscripten generates glue code to interface with the compiled WebAssembly, like
supporting type like strings perhaps instead of WASM's private types.

Now, he command example is written in c++ and this will be compiled by Emscripten to WebAssembly:
```console
$ make -j8 libcommand
```
Lets take a look at what is generated by this:
```console
$ ls bin/command.wasm/
command.js      helpers.js      index.htm

$ ls bin/libcommand.js
bin/libcommand.js
```
So this was a little surprising to me. I expected to see a `.wasm` file but instead I see a `.js` file.
But is is a JavaScript file that contains the WebAssembly binary data as a base64 encoded string.

```console
$ emcmake cmake .. -DWHISPER_WASM_SINGLE_FILE=OFF
$ make -j8 libcommand
$ ls bin/libcommand.wasm
bin/libcommand.wasm
```
This allows us to use tools like wasm-tools to inspect the WebAssembly binary.
```console
$ wasm-tools objdump bin/libcommand.wasm
  types                                  |        0xb -      0x291 |       646 bytes | 82 count
  imports                                |      0x294 -      0x3bb |       295 bytes | 48 count
  functions                              |      0x3be -      0x817 |      1113 bytes | 1111 count
  tables                                 |      0x819 -      0x820 |         7 bytes | 1 count
  globals                                |      0x822 -      0x853 |        49 bytes | 9 count
  exports                                |      0x855 -      0x8b9 |       100 bytes | 18 count
  start                                  |      0x8bb -      0x8bd |         2 bytes | 1 count
  elements                               |      0x8c0 -      0xdbc |      1276 bytes | 1 count
  data count                             |      0xdbe -      0xdbf |         1 bytes | 1 count
  code                                   |      0xdc3 -    0xba020 |    758365 bytes | 1111 count
  data                                   |    0xba024 -    0xda640 |    132636 bytes | 36 count
```

### Running
To run web assembly we need to serve the files over HTTP. This is because the browser enforces the same-origin policy.
And the example also uses web workers which require a server to work.
```console
$ python3 -m http.server 8000 --directory bin
```
And then we can open the browser to the following URL:
http://localhost:8000/command.wasm/

### Installing Emscripten
```console
$ git clone https://github.com/emscripten-core/emsdk.git
$ cd emsdk
$ ./emsdk install latest
$ ./emsdk activate latest
$ source ./emsdk_env.sh
```


### command.wasm example walkthrough
So if we look in the source file for the example, the c++ sources that this, which is
what will be compiled into a .wasm module.
```c++
EMSCRIPTEN_BINDINGS(command) {

    emscripten::function("init", emscripten::optional_override([](const std::string & path_model) {
        for (size_t i = 0; i < g_contexts.size(); ++i) {
            if (g_contexts[i] == nullptr) {
                g_contexts[i] = whisper_init_from_file_with_params(path_model.c_str(), whisper_context_default_params());
                if (g_contexts[i] != nullptr) {
                    g_running = true;
                    if (g_worker.joinable()) {
                        g_worker.join();
                    }
                    g_worker = std::thread([i]() {
                        command_main(i);
                    });

                    return i + 1;
                } else {
                    return (size_t) 0;
                }
            }
        }

        return (size_t) 0;
    }));
}
```
So lets start by asking what is `EMSCRIPTEN_BINDINGS`?
This is a macro that creates a JavaScript binding for the C++ code in side of it. Recall
that WebAssembly only have four basic types: integers, floats, strings, and arrays. What
the binding does is it allows functions to use complex types and have them marshalled to
and from the WebAssembly module. This is similar to wasm-bindgen in Rust.

The `command` is like a namespace for this block.

The `emscripten::function` registers a function enabling it to be called by JavaScript.

When the 'Start' button is clicked this will call the following function:
``html
            <div id="input">
                <button id="start" onclick="onStart()" disabled>Start</button>
                <button id="stop"  onclick="onStop()" disabled>Stop</button>
                <button id="clear" onclick="clearCache()">Clear Cache</button>
            </div>
```
```js
            function onStart() {
                if (!instance) {
                    instance = Module.init('whisper.bin');

                    if (instance) {
                        printTextarea("js: whisper initialized, instance: " + instance);
                    }
                }
```
Where `whisper.bin` it the model that was loaded from the database.
And we can see that the init function is calling `whisper_init_from_file_with_params`
And one thread will be will be created and started if one is available and it will be
started and that new thread will call `command_main` with the index of the context.
```c++
void command_main(size_t index) {
    command_set_status("loading data ...");

    struct whisper_full_params wparams = whisper_full_default_params(whisper_sampling_strategy::WHISPER_SAMPLING_GREEDY);
```

```c++
void command_set_status(const std::string & status) {
    std::lock_guard<std::mutex> lock(g_mutex);
    g_status = status;
}
```

### WASM SIMD issue
When going through the command.wasm example I noticed these warnings when compiling:
```console
/Users/danbev/work/ai/whisper-work/ggml/src/ggml-cpu/ggml-cpu.c:1592:5: warning: implicit conversion increases floating-point precision: 'float' to 'ggml_float' (aka 'double') [-Wdouble-promotion]
 1592 |     GGML_F16_VEC_REDUCE(sumf, sum);
      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/Users/danbev/work/ai/whisper-work/ggml/src/ggml-cpu/ggml-cpu.c:932:37: note: expanded from macro 'GGML_F16_VEC_REDUCE'
  932 | #define GGML_F16_VEC_REDUCE         GGML_F16x4_REDUCE
      |                                     ^
/Users/danbev/work/ai/whisper-work/ggml/src/ggml-cpu/ggml-cpu.c:920:44: note: expanded from macro 'GGML_F16x4_REDUCE'
  918 |     res = wasm_f32x4_extract_lane(x[0], 0) +       \
      |         ~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  919 |           wasm_f32x4_extract_lane(x[0], 1) +       \
      |           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  920 |           wasm_f32x4_extract_lane(x[0], 2) +       \
      |           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~
  921 |           wasm_f32x4_extract_lane(x[0], 3);        \
      |           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/Users/danbev/work/ai/whisper-work/ggml/src/ggml-cpu/ggml-cpu.c:1640:9: warning: implicit conversion increases floating-point precision: 'float' to 'ggml_float' (aka 'double') [-Wdouble-promotion]
 1640 |         GGML_F16_VEC_REDUCE(sumf[k], sum[k]);
      |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/Users/danbev/work/ai/whisper-work/ggml/src/ggml-cpu/ggml-cpu.c:932:37: note: expanded from macro 'GGML_F16_VEC_REDUCE'
  932 | #define GGML_F16_VEC_REDUCE         GGML_F16x4_REDUCE
      |                                     ^
/Users/danbev/work/ai/whisper-work/ggml/src/ggml-cpu/ggml-cpu.c:920:44: note: expanded from macro 'GGML_F16x4_REDUCE'
  918 |     res = wasm_f32x4_extract_lane(x[0], 0) +       \
      |         ~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  919 |           wasm_f32x4_extract_lane(x[0], 1) +       \
      |           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  920 |           wasm_f32x4_extract_lane(x[0], 2) +       \
      |           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~
  921 |           wasm_f32x4_extract_lane(x[0], 3);        \
      |           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2 warnings generated.
```
Now, is we look at the origin it is in ggml/src/ggml-cpu/ggml-cpu.c:
```c++
    // reduce sum0..sum3 to sum0
    GGML_F16_VEC_REDUCE(sumf, sum);
```
This macro expands to:
```c++
#define GGML_F16_VEC_REDUCE         GGML_F16x4_REDUCE

#define GGML_F16x4_REDUCE(res, x)                  \
{                                                  \
    int offset = GGML_F16_ARR >> 1;                \
    for (int i = 0; i < offset; ++i) {             \
        x[i] = wasm_f32x4_add(x[i], x[offset+i]);  \
    }                                              \
    offset >>= 1;                                  \
    for (int i = 0; i < offset; ++i) {             \
        x[i] = wasm_f32x4_add(x[i], x[offset+i]);  \
    }                                              \
    offset >>= 1;                                  \
    for (int i = 0; i < offset; ++i) {             \
        x[i] = wasm_f32x4_add(x[i], x[offset+i]);  \
    }                                              \
    res = wasm_f32x4_extract_lane(x[0], 0) +       \
          wasm_f32x4_extract_lane(x[0], 1) +       \
          wasm_f32x4_extract_lane(x[0], 2) +       \
          wasm_f32x4_extract_lane(x[0], 3);        \
}
```
So in WASM SIMD v128 is the type for vectors, so we can have multiple values in this
as long as they fit, like 4 32 bit ints, or 2 64 bit doubles. This is also called a `lane`,
that is there will be 4 lanes for the intes, and 2 lanes for the doubles.

The operations have the following scheme: `wasm_<type><size>_<operation>` so what we
have above is `wasm_f32x4_extract_lane`. So this is extracting each or the 4 lanes
and adding them.

Now, `wasm_f32x4_extract_lane` returns a 32-bit float into res which is `sumf` which
was defined as:
```c++
    ggml_float sumf = 0.0;
```
And `ggml_float` is of type `typedef double ggml_float`. So `ggml_float` is a double which
is most often 64 bits. So the warning is that we are converting a 32 bit float to a 64 bit
but why would that be an issue. Actually the warning is not about any loss of precision
or loss of data, but it has to do with `double promotion`. Operations on doubles are
more expensive than operations on floats. So the warning is about the fact that we are
promoting the float to a double and that this might be more expensive than we want.

```c++
    ggml_float sumf = 0.0;
    GGML_F16_VEC sum[GGML_VEC_DOT_UNROLL][GGML_F16_ARR] = { { GGML_F16_VEC_ZERO } };
    GGML_F16_VEC_REDUCE(sumf, sum);
```
So we are going from sum which is `GGML_F16_VEC` which is a vector of 4 floats and
we want to extract those values into a single `ggml_float` which is a double.
Where is the issue?
```c++
    res = wasm_f32x4_extract_lane(x[0], 0) +       \
          wasm_f32x4_extract_lane(x[0], 1) +       \
          wasm_f32x4_extract_lane(x[0], 2) +       \
          wasm_f32x4_extract_lane(x[0], 3);        \
```
`wasm_f32x4_extract_lane` returns a 32-bit float and this is what the
addition is performed on. But there is an implicit conversion from
32-bit float to 64-bit double when the result is assigned to `res`,
which is of type `ggml_float`. My understanding here is that this is
intentional, that this is taking four (lanes) 32-bit floats from the
SIMD vector, adding them together and then storing the result as a
single 64-bit float.
